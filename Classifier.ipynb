{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-08T20:54:54.223014Z",
     "iopub.status.busy": "2025-02-08T20:54:54.222626Z",
     "iopub.status.idle": "2025-02-08T20:54:54.234758Z",
     "shell.execute_reply": "2025-02-08T20:54:54.233695Z",
     "shell.execute_reply.started": "2025-02-08T20:54:54.222984Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all necessary modules and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T20:55:00.325108Z",
     "iopub.status.busy": "2025-02-08T20:55:00.324664Z",
     "iopub.status.idle": "2025-02-08T20:55:08.793816Z",
     "shell.execute_reply": "2025-02-08T20:55:08.792693Z",
     "shell.execute_reply.started": "2025-02-08T20:55:00.325073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install contractions\n",
    "!pip install word2number\n",
    "\n",
    "import re, string, subprocess, emoji, nltk, inflect, contractions, json\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import WordCloud\n",
    "from word2number import w2n\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('/kaggle/input/my_stopwords.txt', 'r') as f:\n",
    "    custom_stopwords = set(f.read().splitlines())\n",
    "\n",
    "with open(\"/kaggle/input/genz_dict.txt\", \"r\") as file:\n",
    "    genz_dict = json.load(file)\n",
    "\n",
    "with open(\"/kaggle/input/emoticon_dict.txt\", \"r\") as file:\n",
    "    emoticon_dict = json.load(file)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('wordnet.zip')\n",
    "except:\n",
    "    nltk.download('wordnet', download_dir='/kaggle/working/')\n",
    "    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n",
    "    subprocess.run(command.split())\n",
    "    nltk.data.path.append('/kaggle/working/')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dropping first column of indices as it was redundant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T20:58:31.056996Z",
     "iopub.status.busy": "2025-02-08T20:58:31.056557Z",
     "iopub.status.idle": "2025-02-08T20:58:32.237197Z",
     "shell.execute_reply": "2025-02-08T20:58:32.236311Z",
     "shell.execute_reply.started": "2025-02-08T20:58:31.056963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/kaggle/input/movie-review-train-data/Movie_reviews_non_comp.csv\")\n",
    "dataset = dataset.drop(dataset.columns[0], axis=1)\n",
    "sample = dataset.head()\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# see class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:24:37.367806Z",
     "iopub.status.busy": "2025-02-08T21:24:37.367459Z",
     "iopub.status.idle": "2025-02-08T21:24:37.400492Z",
     "shell.execute_reply": "2025-02-08T21:24:37.399545Z",
     "shell.execute_reply.started": "2025-02-08T21:24:37.367781Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(dataset['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**removed html and url tags**\n",
    "\n",
    "**converted emojis and emoticons to text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:00:26.256593Z",
     "iopub.status.busy": "2025-02-08T21:00:26.256091Z",
     "iopub.status.idle": "2025-02-08T21:01:24.128730Z",
     "shell.execute_reply": "2025-02-08T21:01:24.127601Z",
     "shell.execute_reply.started": "2025-02-08T21:00:26.256556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset['clean_review'] = dataset['Review']\n",
    "# remove html and url tags\n",
    "dataset['clean_review'] = dataset['clean_review'].apply(lambda x: re.sub(r'http\\S+|www.\\S+', '', x))\n",
    "dataset['clean_review'] = dataset['clean_review'].apply(lambda x: BeautifulSoup(x, \"lxml\").text)\n",
    "\n",
    "def convert_emojis_to_words(text):\n",
    "    converted_text = emoji.demojize(text)\n",
    "    return converted_text\n",
    "\n",
    "def convert_emoticon_to_words(text):\n",
    "    for emoticon,desc in emoticon_dict.items():\n",
    "        converted_text=text.replace(emoticon,desc)\n",
    "    return converted_text\n",
    "\n",
    "\n",
    "dataset['clean_review'] = dataset['clean_review'].apply(convert_emojis_to_words)\n",
    "dataset['clean_review'] = dataset['clean_review'].apply(convert_emoticon_to_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**converted all text to lowercase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:01:24.130280Z",
     "iopub.status.busy": "2025-02-08T21:01:24.129975Z",
     "iopub.status.idle": "2025-02-08T21:01:24.384167Z",
     "shell.execute_reply": "2025-02-08T21:01:24.383022Z",
     "shell.execute_reply.started": "2025-02-08T21:01:24.130253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset['clean_review'] = dataset['Review'].apply(lambda x: x.lower())\n",
    "print(dataset['clean_review'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**we noticed that some review contain fraction where people gave rating like 4/5 . So we tried to capture there sentiment here by converting it to good/bad**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:02:43.882005Z",
     "iopub.status.busy": "2025-02-08T21:02:43.881579Z",
     "iopub.status.idle": "2025-02-08T21:02:45.693767Z",
     "shell.execute_reply": "2025-02-08T21:02:45.692649Z",
     "shell.execute_reply.started": "2025-02-08T21:02:43.881970Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fraction_to_sentiment(review):\n",
    "    pattern = r'(\\d+(\\.\\d+)?)/(\\d+(\\.\\d+)?)'\n",
    "    matches = re.findall(pattern, review)\n",
    "    \n",
    "    for match in matches:\n",
    "        num_str, _, denom_str, _ = match\n",
    "        \n",
    "        num = float(num_str)\n",
    "        denom = float(denom_str)\n",
    "        \n",
    "        if num == 9 and denom == 11:\n",
    "            continue\n",
    "\n",
    "        if denom == 0:\n",
    "            continue\n",
    "        \n",
    "        result = num / denom\n",
    "        \n",
    "        if result > 0.8:\n",
    "            replacement = \"awesome\"\n",
    "        elif 0.6 <= result <= 0.8:\n",
    "            replacement = \"good\"\n",
    "        elif 0.4 <= result < 0.6:\n",
    "            replacement = \"neutral\"\n",
    "        elif 0.2 <= result < 0.4:\n",
    "            replacement = \"bad\"\n",
    "        else:\n",
    "            replacement = \"worst\"\n",
    "        \n",
    "        review = review.replace(f\"{num_str}/{denom_str}\", replacement)\n",
    "    \n",
    "    return review\n",
    "\n",
    "dataset['clean_review'] = dataset['clean_review'].apply(fraction_to_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**converted genz text lingo to embedding readable form**\n",
    "\n",
    "**expanded contractions like dont't to do not**\n",
    "\n",
    "**removed punctuations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:03:11.236105Z",
     "iopub.status.busy": "2025-02-08T21:03:11.235715Z",
     "iopub.status.idle": "2025-02-08T21:03:22.773570Z",
     "shell.execute_reply": "2025-02-08T21:03:22.772528Z",
     "shell.execute_reply.started": "2025-02-08T21:03:11.236075Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_genz_lingo(text):\n",
    "    words=text.split()\n",
    "    new_words=[]\n",
    "    for word in words:\n",
    "        if word in genz_dict:\n",
    "            new_words.append(genz_dict[word])\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "dataset['clean_review'] = dataset['clean_review'].apply(convert_genz_lingo)\n",
    "dataset['clean_review'] = dataset['clean_review'].apply(lambda x: contractions.fix(x))\n",
    "dataset['clean_review'] = dataset['clean_review'].apply(lambda x: re.sub(r\"[^\\w\\s]\", '', x))\n",
    "\n",
    "print(dataset['clean_review'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:03:22.897170Z",
     "iopub.status.busy": "2025-02-08T21:03:22.896814Z",
     "iopub.status.idle": "2025-02-08T21:04:28.866584Z",
     "shell.execute_reply": "2025-02-08T21:04:28.865575Z",
     "shell.execute_reply.started": "2025-02-08T21:03:22.897143Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset['tokens'] = dataset['clean_review'].apply(lambda x: word_tokenize(x))\n",
    "print(dataset['tokens'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**wordcloud before removing stop words and lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T20:33:38.635856Z",
     "iopub.status.busy": "2025-02-08T20:33:38.635500Z",
     "iopub.status.idle": "2025-02-08T20:33:46.516357Z",
     "shell.execute_reply": "2025-02-08T20:33:46.515263Z",
     "shell.execute_reply.started": "2025-02-08T20:33:38.635823Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_tokens = [token for tokens_list in dataset['tokens'] for token in tokens_list]\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(nltk.FreqDist(all_tokens)))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word length analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T20:33:46.517673Z",
     "iopub.status.busy": "2025-02-08T20:33:46.517395Z",
     "iopub.status.idle": "2025-02-08T20:34:11.552583Z",
     "shell.execute_reply": "2025-02-08T20:34:11.551443Z",
     "shell.execute_reply.started": "2025-02-08T20:33:46.517648Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "word_lengths = [len(word) for word in all_tokens]  \n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(word_lengths, bins=range(1, max(word_lengths) + 2), edgecolor=\"black\", alpha=0.7)\n",
    "plt.xlabel(\"Sentence Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Sentence Lengths\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T20:34:11.555905Z",
     "iopub.status.busy": "2025-02-08T20:34:11.555595Z",
     "iopub.status.idle": "2025-02-08T20:34:13.183520Z",
     "shell.execute_reply": "2025-02-08T20:34:13.182430Z",
     "shell.execute_reply.started": "2025-02-08T20:34:11.555878Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "longest_word = max(all_tokens, key=len) \n",
    "print(f\"Longest word: {longest_word} (Length: {len(longest_word)})\")\n",
    "avg_word_length = np.mean([len(word) for word in all_tokens])\n",
    "print(f\"Average word length: {avg_word_length:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**character frequency analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T20:34:13.185733Z",
     "iopub.status.busy": "2025-02-08T20:34:13.185447Z",
     "iopub.status.idle": "2025-02-08T20:34:16.632604Z",
     "shell.execute_reply": "2025-02-08T20:34:16.631471Z",
     "shell.execute_reply.started": "2025-02-08T20:34:13.185710Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "char_counts = Counter(\"\".join(all_tokens))\n",
    "sorted_chars = sorted(char_counts.keys())  \n",
    "sorted_counts = [char_counts[ch] for ch in sorted_chars]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(sorted_chars, sorted_counts, color=\"skyblue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.xlabel(\"Characters\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Character Frequency Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sentence length analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:53:15.431874Z",
     "iopub.status.busy": "2025-02-08T21:53:15.431470Z",
     "iopub.status.idle": "2025-02-08T21:53:15.833949Z",
     "shell.execute_reply": "2025-02-08T21:53:15.832626Z",
     "shell.execute_reply.started": "2025-02-08T21:53:15.431842Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "token_counts = dataset[\"tokens\"].apply(len)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(token_counts, bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.title(\"Histogram of Sentence lengths\", fontsize=16)\n",
    "plt.xlabel(\"Number of words\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T22:04:50.217451Z",
     "iopub.status.busy": "2025-02-08T22:04:50.217010Z",
     "iopub.status.idle": "2025-02-08T22:04:50.224894Z",
     "shell.execute_reply": "2025-02-08T22:04:50.223814Z",
     "shell.execute_reply.started": "2025-02-08T22:04:50.217417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "avg_sentence_length = np.mean([token_counts])\n",
    "print(f\"Average sentence length: {avg_sentence_length:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:04:32.395912Z",
     "iopub.status.busy": "2025-02-08T21:04:32.395548Z",
     "iopub.status.idle": "2025-02-08T21:04:37.182859Z",
     "shell.execute_reply": "2025-02-08T21:04:37.181760Z",
     "shell.execute_reply.started": "2025-02-08T21:04:32.395883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset['tokens'] = dataset['tokens'].apply(lambda x: [word for word in x if word not in custom_stopwords])\n",
    "print(dataset['tokens'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**converting numbers to word form**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:04:37.184437Z",
     "iopub.status.busy": "2025-02-08T21:04:37.184043Z",
     "iopub.status.idle": "2025-02-08T21:04:56.310240Z",
     "shell.execute_reply": "2025-02-08T21:04:56.309352Z",
     "shell.execute_reply.started": "2025-02-08T21:04:37.184392Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_numbers_to_words(tokens):\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            word = w2n.word_to_num(token)\n",
    "            new_tokens.append(str(word))  \n",
    "        except ValueError:\n",
    "            new_tokens.append(token)\n",
    "    return new_tokens\n",
    "\n",
    "def convert_digits_to_words(tokens):\n",
    "    return [inflect.engine().number_to_words(token) if token.isdigit() else token for token in tokens]\n",
    "\n",
    "dataset['tokens'] = dataset['tokens'].apply(convert_numbers_to_words)\n",
    "dataset['tokens'] = dataset['tokens'].apply(convert_digits_to_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:04:56.311981Z",
     "iopub.status.busy": "2025-02-08T21:04:56.311671Z",
     "iopub.status.idle": "2025-02-08T21:11:38.077925Z",
     "shell.execute_reply": "2025-02-08T21:11:38.076921Z",
     "shell.execute_reply.started": "2025-02-08T21:04:56.311951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    pos_tags = nltk.pos_tag(text)\n",
    "    lemmatized_words = []\n",
    "    for word, tag in pos_tags:\n",
    "        pos = wordnet_map.get(tag[0].upper(), wordnet.NOUN)\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, pos=pos)\n",
    "        lemmatized_words.append(lemmatized_word)\n",
    "    return lemmatized_words\n",
    "\n",
    "dataset['tokens'] = dataset['tokens'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordcloud after data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:11:38.079611Z",
     "iopub.status.busy": "2025-02-08T21:11:38.079331Z",
     "iopub.status.idle": "2025-02-08T21:11:43.385696Z",
     "shell.execute_reply": "2025-02-08T21:11:43.384467Z",
     "shell.execute_reply.started": "2025-02-08T21:11:38.079588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_tokens = [token for tokens_list in dataset['tokens'] for token in tokens_list]\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(nltk.FreqDist(all_tokens)))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams after data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:11:43.387304Z",
     "iopub.status.busy": "2025-02-08T21:11:43.386928Z",
     "iopub.status.idle": "2025-02-08T21:11:56.087503Z",
     "shell.execute_reply": "2025-02-08T21:11:56.086435Z",
     "shell.execute_reply.started": "2025-02-08T21:11:43.387264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "unigrams = all_tokens\n",
    "bigrams = list(ngrams(all_tokens, 2))\n",
    "trigrams = list(ngrams(all_tokens, 3))\n",
    "\n",
    "unigram_counts = Counter(unigrams)\n",
    "bigram_counts = Counter(bigrams)\n",
    "trigram_counts = Counter(trigrams)\n",
    "\n",
    "top_unigrams = unigram_counts.most_common(10)\n",
    "top_bigrams = bigram_counts.most_common(10)\n",
    "top_trigrams = trigram_counts.most_common(10)\n",
    "\n",
    "labels_uni, values_uni = zip(*top_unigrams)\n",
    "labels_bi, values_bi = zip(*[(\" \".join(bi), count) for bi, count in top_bigrams])\n",
    "labels_tri, values_tri = zip(*[(\" \".join(tri), count) for tri, count in top_trigrams])\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12,18))\n",
    "axs[0].bar(labels_uni, values_uni, color='skyblue')\n",
    "axs[0].set_title(\"Top Unigrams\")\n",
    "axs[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axs[1].bar(labels_bi, values_bi, color='lightgreen')\n",
    "axs[1].set_title(\"Top Bigrams\")\n",
    "axs[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axs[2].bar(labels_tri, values_tri, color='salmon')\n",
    "axs[2].set_title(\"Top Trigrams\")\n",
    "axs[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading word embedding and check coverage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**we used google word2vec 300 dimensional embedding trained on google news** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:11:56.088982Z",
     "iopub.status.busy": "2025-02-08T21:11:56.088642Z",
     "iopub.status.idle": "2025-02-08T21:12:00.008205Z",
     "shell.execute_reply": "2025-02-08T21:12:00.006976Z",
     "shell.execute_reply.started": "2025-02-08T21:11:56.088949Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "path_to_model = '/kaggle/input/GoogleNews-vectors-negative300.bin' \n",
    "word2vec = KeyedVectors.load_word2vec_format(path_to_model, binary=True, limit=100000)\n",
    "\n",
    "def check_coverage(tokenized_reviews, model):\n",
    "    word_counts = Counter()\n",
    "    total_words = 0\n",
    "    known_words = 0\n",
    "\n",
    "    for tokens in tokenized_reviews:  \n",
    "        word_counts.update(tokens)  \n",
    "\n",
    "    total_words = sum(word_counts.values())  \n",
    "    known_words = sum(count for word, count in word_counts.items() if word in model)\n",
    "\n",
    "    print(f\"Total words in dataset: {total_words}\")\n",
    "    print(f\"Words covered in Word2Vec: {known_words} ({(known_words / total_words) * 100:.2f}%)\")\n",
    "\n",
    "check_coverage(dataset['tokens'], word2vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lets see top out of vocabulary words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:12:00.010818Z",
     "iopub.status.busy": "2025-02-08T21:12:00.010489Z",
     "iopub.status.idle": "2025-02-08T21:12:03.521560Z",
     "shell.execute_reply": "2025-02-08T21:12:03.520451Z",
     "shell.execute_reply.started": "2025-02-08T21:12:00.010788Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "oov_words = [word for tokens in dataset['tokens'] for word in tokens if word not in word2vec]\n",
    "oov_word_counts = Counter(oov_words)\n",
    "\n",
    "print(\"Most frequent Out of Vocabulary words:\")\n",
    "print(oov_word_counts.most_common(20)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:12:03.523550Z",
     "iopub.status.busy": "2025-02-08T21:12:03.523189Z",
     "iopub.status.idle": "2025-02-08T21:12:03.527365Z",
     "shell.execute_reply": "2025-02-08T21:12:03.526454Z",
     "shell.execute_reply.started": "2025-02-08T21:12:03.523520Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting sentences to fixed size vectors using word embedding\n",
    "**with avg , sum and max norm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:12:03.528826Z",
     "iopub.status.busy": "2025-02-08T21:12:03.528457Z",
     "iopub.status.idle": "2025-02-08T21:13:02.548775Z",
     "shell.execute_reply": "2025-02-08T21:13:02.547809Z",
     "shell.execute_reply.started": "2025-02-08T21:12:03.528788Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_vector(tokenized_sentence, model, method=\"average\"):\n",
    "    words = [word for word in tokenized_sentence if word in model] \n",
    "    if not words:\n",
    "        return np.zeros(300) \n",
    "        \n",
    "    word_vectors = np.array([model[word] for word in words])  \n",
    "    \n",
    "    if method == \"sum\":\n",
    "        return np.sum(word_vectors, axis=0)  \n",
    "    elif method == \"max\":\n",
    "        return np.max(word_vectors, axis=0)  \n",
    "    else:\n",
    "        return np.mean(word_vectors, axis=0)  \n",
    "\n",
    "df['sentence_vectors_avg'] = [sentence_to_vector(tokens, word2vec) for tokens in tqdm(df['tokens'], desc=\"Vectorizing Sentences with avg norm\")]\n",
    "df['sentence_vectors_sum'] = [sentence_to_vector(tokens, word2vec,\"sum\") for tokens in tqdm(df['tokens'], desc=\"Vectorizing Sentences with sum norm\")]\n",
    "df['sentence_vectors_max'] = [sentence_to_vector(tokens, word2vec,\"max\") for tokens in tqdm(df['tokens'], desc=\"Vectorizing Sentences with max norm\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test split (stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:13:02.550181Z",
     "iopub.status.busy": "2025-02-08T21:13:02.549803Z",
     "iopub.status.idle": "2025-02-08T21:13:07.349261Z",
     "shell.execute_reply": "2025-02-08T21:13:07.348445Z",
     "shell.execute_reply.started": "2025-02-08T21:13:02.550134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_avg = np.array(df['sentence_vectors_avg'].tolist())  \n",
    "X_sum = np.array(df['sentence_vectors_sum'].tolist())  \n",
    "X_max = np.array(df['sentence_vectors_max'].tolist())  \n",
    "y = df['Class']\n",
    "X_avg_train, X_avg_test, y_avg_train, y_avg_test = train_test_split(X_avg, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_sum_train, X_sum_test, y_sum_train, y_sum_test = train_test_split(X_sum, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_max_train, X_max_test, y_max_train, y_max_test = train_test_split(X_max, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch gradient descent \n",
    "**will do it 3 times , ones for each avg, sum and max norm sentence vector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**with avg norm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:13:07.350561Z",
     "iopub.status.busy": "2025-02-08T21:13:07.350275Z",
     "iopub.status.idle": "2025-02-08T21:14:05.741195Z",
     "shell.execute_reply": "2025-02-08T21:14:05.740131Z",
     "shell.execute_reply.started": "2025-02-08T21:13:07.350538Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "clf_batch_avg = LogisticRegression(max_iter=1000, multi_class='ovr', solver='lbfgs') \n",
    "clf_batch_avg.fit(X_avg_train, y_avg_train)\n",
    "y_avg_pred_test = clf_batch_avg.predict(X_avg_test)\n",
    "y_avg_pred_train = clf_batch_avg.predict(X_avg_train)\n",
    "\n",
    "print(f\"Accuracy_on_test_data: {accuracy_score(y_avg_test, y_avg_pred_test):.4f}\")\n",
    "print(\"Classification Report test data:\")\n",
    "print(classification_report(y_avg_test, y_avg_pred_test))\n",
    "\n",
    "print(f\"Accuracy_on_train_data: {accuracy_score(y_avg_train, y_avg_pred_train):.4f}\")\n",
    "print(\"Classification Report train data:\")\n",
    "print(classification_report(y_avg_train, y_avg_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**with sum norm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:14:05.742739Z",
     "iopub.status.busy": "2025-02-08T21:14:05.742324Z",
     "iopub.status.idle": "2025-02-08T21:15:01.447145Z",
     "shell.execute_reply": "2025-02-08T21:15:01.445963Z",
     "shell.execute_reply.started": "2025-02-08T21:14:05.742702Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "clf_batch_sum = LogisticRegression(max_iter=1000, multi_class='ovr', solver='lbfgs') \n",
    "clf_batch_sum.fit(X_sum_train, y_sum_train)\n",
    "y_sum_pred_test = clf_batch_sum.predict(X_sum_test)\n",
    "y_sum_pred_train = clf_batch_sum.predict(X_sum_train)\n",
    "\n",
    "print(f\"Accuracy_on_test_data: {accuracy_score(y_sum_test, y_sum_pred_test):.4f}\")\n",
    "print(\"Classification Report test data:\")\n",
    "print(classification_report(y_sum_test, y_sum_pred_test))\n",
    "\n",
    "print(f\"Accuracy_on_train_data: {accuracy_score(y_sum_train, y_sum_pred_train):.4f}\")\n",
    "print(\"Classification Report train data:\")\n",
    "print(classification_report(y_sum_train, y_sum_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**with max norm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:15:01.448435Z",
     "iopub.status.busy": "2025-02-08T21:15:01.448134Z",
     "iopub.status.idle": "2025-02-08T21:15:48.251156Z",
     "shell.execute_reply": "2025-02-08T21:15:48.250208Z",
     "shell.execute_reply.started": "2025-02-08T21:15:01.448410Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "clf_batch_max = LogisticRegression(max_iter=1000, multi_class='ovr', solver='lbfgs') \n",
    "clf_batch_max.fit(X_max_train, y_max_train)\n",
    "y_max_pred_test = clf_batch_max.predict(X_max_test)\n",
    "y_max_pred_train = clf_batch_max.predict(X_max_train)\n",
    "\n",
    "print(f\"Accuracy_on_test_data: {accuracy_score(y_max_test, y_max_pred_test):.4f}\")\n",
    "print(\"Classification Report test data:\")\n",
    "print(classification_report(y_max_test, y_max_pred_test))\n",
    "\n",
    "print(f\"Accuracy_on_train_data: {accuracy_score(y_max_train, y_max_pred_train):.4f}\")\n",
    "print(\"Classification Report train data:\")\n",
    "print(classification_report(y_max_train, y_max_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**as all 3 norms give similar accuracy, with avg norm being slightly better, we will use avg norm from now on**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:15:48.252454Z",
     "iopub.status.busy": "2025-02-08T21:15:48.252147Z",
     "iopub.status.idle": "2025-02-08T21:15:49.174917Z",
     "shell.execute_reply": "2025-02-08T21:15:49.173788Z",
     "shell.execute_reply.started": "2025-02-08T21:15:48.252427Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_avg, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning to find number of iterations at which convergence was achieved**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:15:49.176364Z",
     "iopub.status.busy": "2025-02-08T21:15:49.175975Z",
     "iopub.status.idle": "2025-02-08T21:18:42.009984Z",
     "shell.execute_reply": "2025-02-08T21:18:42.008934Z",
     "shell.execute_reply.started": "2025-02-08T21:15:49.176303Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_iter_values = [100,300,500,1000] \n",
    "accuracies = []\n",
    "for max_iter_value in max_iter_values:\n",
    "    clf_batch = LogisticRegression(max_iter=max_iter_value, multi_class='ovr', solver='lbfgs')\n",
    "    clf_batch.fit(X_train, y_train)\n",
    "    y_pred_test = clf_batch.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"max_iter = {max_iter_value}, Accuracy on test data: {accuracy:.4f}\")\n",
    "\n",
    "plt.plot(max_iter_values, accuracies, marker='o')\n",
    "plt.xlabel('max_iter')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Hyperparameter Tuning: max_iter vs Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:18:42.011280Z",
     "iopub.status.busy": "2025-02-08T21:18:42.010971Z",
     "iopub.status.idle": "2025-02-08T21:19:12.098841Z",
     "shell.execute_reply": "2025-02-08T21:19:12.097793Z",
     "shell.execute_reply.started": "2025-02-08T21:18:42.011245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "clf_stochastic = SGDClassifier(\n",
    "    loss='log_loss',\n",
    "    max_iter=1000,\n",
    "    tol=1e-4,\n",
    "    random_state=42,\n",
    "    penalty='l2',\n",
    "    alpha=0.0001 \n",
    ")\n",
    "\n",
    "clf_stochastic.fit(X_train, y_train)\n",
    "y_pred_test_stochastic = clf_stochastic.predict(X_test)\n",
    "y_pred_train_stochastic = clf_stochastic.predict(X_train)\n",
    "\n",
    "print(f\"Accuracy_on_test_data: {accuracy_score(y_test, y_pred_test_stochastic):.4f}\")\n",
    "print(\"Classification Report test data:\")\n",
    "print(classification_report(y_test, y_pred_test_stochastic))\n",
    "\n",
    "print(f\"Accuracy_on_train_data: {accuracy_score(y_train, y_pred_train_stochastic):.4f}\")\n",
    "print(\"Classification Report train data:\")\n",
    "print(classification_report(y_train, y_pred_train_stochastic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning to find number of iterations at which convergence was achieved**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:19:12.100046Z",
     "iopub.status.busy": "2025-02-08T21:19:12.099773Z",
     "iopub.status.idle": "2025-02-08T21:21:38.308281Z",
     "shell.execute_reply": "2025-02-08T21:21:38.307227Z",
     "shell.execute_reply.started": "2025-02-08T21:19:12.100022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_iter_values = [10,30,50,100,300,500,1000]\n",
    "accuracies = []\n",
    "\n",
    "for max_iter_value in max_iter_values:\n",
    "    clf_stochastic = SGDClassifier(\n",
    "    loss='log_loss',          \n",
    "    max_iter=max_iter_value,       \n",
    "    tol=1e-4,\n",
    "    random_state=42,\n",
    "    penalty='l2',\n",
    "    alpha=0.0001\n",
    "    )\n",
    "\n",
    "    clf_stochastic.fit(X_train, y_train)\n",
    "    y_pred_test_stochastic = clf_stochastic.predict(X_test)    \n",
    "    accuracy = accuracy_score(y_test, y_pred_test_stochastic)\n",
    "    accuracies.append(accuracy)    \n",
    "    print(f\"max_iter = {max_iter_value}, Accuracy on test data: {accuracy:.4f}\")\n",
    "\n",
    "plt.plot(max_iter_values, accuracies, marker='o')\n",
    "plt.xlabel('max_iter')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Hyperparameter Tuning: max_iter vs Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T21:21:38.311621Z",
     "iopub.status.busy": "2025-02-08T21:21:38.311293Z",
     "iopub.status.idle": "2025-02-08T21:23:28.014720Z",
     "shell.execute_reply": "2025-02-08T21:23:28.013614Z",
     "shell.execute_reply.started": "2025-02-08T21:21:38.311594Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mini_batch_size = 100  \n",
    "n_epochs = 10         \n",
    "clf_minibatch = SGDClassifier(\n",
    "    loss='log_loss',  \n",
    "    max_iter=1,\n",
    "    tol=None,\n",
    "    random_state=42,\n",
    "    penalty='l2',\n",
    "    alpha=0.0001         \n",
    ")\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "clf_minibatch.partial_fit(X_train[:mini_batch_size], y_train[:mini_batch_size], classes=classes)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train, y_train, random_state=42)\n",
    "    for i in range(0, len(X_train), mini_batch_size):\n",
    "        X_mini_batch = X_train_shuffled[i:i + mini_batch_size]\n",
    "        y_mini_batch = y_train_shuffled[i:i + mini_batch_size]\n",
    "        clf_minibatch.partial_fit(X_mini_batch, y_mini_batch)\n",
    "\n",
    "y_pred_test_minibatch = clf_minibatch.predict(X_test)\n",
    "y_pred_train_minibatch = clf_minibatch.predict(X_train)\n",
    "\n",
    "print(f\"Accuracy_on_test_data: {accuracy_score(y_test, y_pred_test_minibatch):.4f}\")\n",
    "print(\"Classification Report test data:\")\n",
    "print(classification_report(y_test, y_pred_test_minibatch))\n",
    "\n",
    "print(f\"Accuracy_on_train_data: {accuracy_score(y_train, y_pred_train_minibatch):.4f}\")\n",
    "print(\"Classification Report train data:\")\n",
    "print(classification_report(y_train, y_pred_train_minibatch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:25:38.132582Z",
     "iopub.status.busy": "2025-02-09T13:25:38.132141Z",
     "iopub.status.idle": "2025-02-09T13:25:38.213399Z",
     "shell.execute_reply": "2025-02-09T13:25:38.211784Z",
     "shell.execute_reply.started": "2025-02-09T13:25:38.132552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier(\n",
    "    n_estimators=100,      \n",
    "    max_depth=10,  \n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,       \n",
    "    criterion='gini',      \n",
    "    n_jobs=-1              \n",
    ")\n",
    "\n",
    "clf_rf.fit(X_train, y_train)\n",
    "y_pred_test_rf = clf_rf.predict(X_test)\n",
    "y_pred_train_rf = clf_rf.predict(X_train)\n",
    "\n",
    "print(f\"Accuracy_on_test_data: {accuracy_score(y_test, y_pred_test_rf):.4f}\")\n",
    "print(\"Classification Report test data:\")\n",
    "print(classification_report(y_test, y_pred_test_rf))\n",
    "\n",
    "print(f\"Accuracy_on_train_data: {accuracy_score(y_train, y_pred_train_rf):.4f}\")\n",
    "print(\"Classification Report train data:\")\n",
    "print(classification_report(y_train, y_pred_train_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning using Randomized Search for random forest classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-08T21:24:26.822264Z",
     "iopub.status.idle": "2025-02-08T21:24:26.825866Z",
     "shell.execute_reply": "2025-02-08T21:24:26.825649Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV,StratifiedShuffleSplit\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 500],  \n",
    "    'max_depth': [10, 20, None],  \n",
    "    'min_samples_split': [2, 5, 10],  \n",
    "    'min_samples_leaf': [1, 2, 4],  \n",
    "    'criterion': ['gini'],  \n",
    "}\n",
    "\n",
    "clf_rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=clf_rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,             \n",
    "    scoring='accuracy',    \n",
    "    cv=sss,                 \n",
    "    verbose=2,             \n",
    "    random_state=42,\n",
    "    n_jobs=-1              \n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(f\"Best Accuracy from Validation Split:  {random_search.best_score_:.4f}\")\n",
    "best_rf = random_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"test accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6526521,
     "sourceId": 10548277,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6546600,
     "sourceId": 10578597,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6547140,
     "sourceId": 10579428,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6629809,
     "sourceId": 10698537,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
